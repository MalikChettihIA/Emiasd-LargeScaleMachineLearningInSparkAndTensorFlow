# VectorAssembler dans Spark ML - Guide Complet

## Vue d'ensemble

**VectorAssembler** est un transformateur fondamental de Spark ML qui combine plusieurs colonnes de features en un seul vecteur. Il s'agit d'une étape obligatoire dans la plupart des pipelines de machine learning avec Spark, car les algorithmes ML requièrent que toutes les features soient regroupées dans une seule colonne de type Vector.

## Principe de fonctionnement

### Objectif principal
VectorAssembler prend en entrée plusieurs colonnes (numériques ou vecteurs) et les concatène pour créer un seul vecteur dans une nouvelle colonne, généralement appelée "features".

### Syntaxe de base
```scala
import org.apache.spark.ml.feature.VectorAssembler

val assembler = new VectorAssembler()
    .setInputCols(Array("colonne1", "colonne2", "colonne3"))
    .setOutputCol("features")

val result = assembler.transform(dataFrame)
```

## Exemple pratique détaillé

### Données d'entrée
```scala
// DataFrame initial avec des colonnes séparées
+----------+-------------+---------+
|indexed_age|indexed_income|student |
+----------+-------------+---------+
|       1.0|          0.0|     0.0|
|       2.0|          1.0|     1.0|
|       0.0|          2.0|     0.0|
+----------+-------------+---------+
```

### Application de VectorAssembler
```scala
val vectorAssembler = new VectorAssembler()
    .setInputCols(Array("indexed_age", "indexed_income", "student"))
    .setOutputCol("features")

val vectorizedData = vectorAssembler.transform(data)
```

### Résultat
```scala
+----------+-------------+---------+-------------+
|indexed_age|indexed_income|student |features     |
+----------+-------------+---------+-------------+
|       1.0|          0.0|     0.0|[1.0,0.0,0.0]|
|       2.0|          1.0|     1.0|[2.0,1.0,1.0]|
|       0.0|          2.0|     0.0|[0.0,2.0,0.0]|
+----------+-------------+---------+-------------+
```

## Gestion des vecteurs sparse et dense

### Avec des vecteurs One-Hot encodés
```scala
// Données après OneHotEncoder (vecteurs sparse)
+-------------+-------------+-------------------+
|categoryVec1 |categoryVec2 |                  |
+-------------+-------------+-------------------+
|(3,[0],[1.0])|(3,[1],[1.0])|                  |
|(3,[1],[1.0])|(3,[0],[1.0])|                  |
|(3,[2],[1.0])|(3,[2],[1.0])|                  |
+-------------+-------------+-------------------+

// Après VectorAssembler
+-------------+-------------+-------------------+
|categoryVec1 |categoryVec2 |features           |
+-------------+-------------+-------------------+
|(3,[0],[1.0])|(3,[1],[1.0])|(6,[0,4],[1.0,1.0])|
|(3,[1],[1.0])|(3,[0],[1.0])|(6,[1,3],[1.0,1.0])|
|(3,[2],[1.0])|(3,[2],[1.0])|(6,[2,5],[1.0,1.0])|
+-------------+-------------+-------------------+
```

### Explication du format sparse
- `(6,[0,4],[1.0,1.0])` signifie :
  - Vecteur de taille 6
  - Positions non-nulles : indices 0 et 4
  - Valeurs : 1.0 aux positions 0 et 4
  - Équivalent dense : `[1.0, 0.0, 0.0, 0.0, 1.0, 0.0]`

## Integration dans un pipeline complet

### Pipeline typique de preprocessing
```scala
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}

// 1. Indexation des variables catégorielles
val ageIndexer = new StringIndexer()
    .setInputCol("age")
    .setOutputCol("indexed_age")

val incomeIndexer = new StringIndexer()
    .setInputCol("income")
    .setOutputCol("indexed_income")

// 2. One-Hot Encoding (optionnel)
val oneHotEncoder = new OneHotEncoder()
    .setInputCols(Array("indexed_age", "indexed_income"))
    .setOutputCols(Array("age_vec", "income_vec"))

// 3. Assemblage des features
val vectorAssembler = new VectorAssembler()
    .setInputCols(Array("age_vec", "income_vec"))
    .setOutputCol("features")

// 4. Pipeline complet
val pipeline = new Pipeline()
    .setStages(Array(ageIndexer, incomeIndexer, oneHotEncoder, vectorAssembler))

val model = pipeline.fit(data)
val processedData = model.transform(data)
```

## Cas d'usage courants

### 1. Combinaison de features numériques simples
```scala
val assembler = new VectorAssembler()
    .setInputCols(Array("age", "salary", "experience"))
    .setOutputCol("features")
```

### 2. Mélange de features numériques et catégorielles indexées
```scala
val assembler = new VectorAssembler()
    .setInputCols(Array("salary", "indexed_department", "indexed_level"))
    .setOutputCol("features")
```

### 3. Combinaison de vecteurs One-Hot
```scala
val assembler = new VectorAssembler()
    .setInputCols(Array("dept_vec", "level_vec", "location_vec"))
    .setOutputCol("features")
```

### 4. Features mixtes (numériques + vecteurs)
```scala
val assembler = new VectorAssembler()
    .setInputCols(Array("age", "salary", "dept_vec", "skills_vec"))
    .setOutputCol("features")
```

## Optimisations et bonnes pratiques

### Gestion automatique sparse/dense
VectorAssembler optimise automatiquement le format :
- **Vecteurs sparse** : Utilisés quand la majorité des valeurs sont nulles
- **Vecteurs dense** : Utilisés quand la plupart des valeurs sont non-nulles
- **Conversion automatique** : Choix du format le plus efficace

### Ordre des features
```scala
// L'ordre dans setInputCols détermine l'ordre dans le vecteur final
val assembler = new VectorAssembler()
    .setInputCols(Array("feat1", "feat2", "feat3"))  // feat1=index 0, feat2=index 1, etc.
    .setOutputCol("features")
```

### Gestion des valeurs manquantes
```scala
// VectorAssembler propage les valeurs NaN
// Il est recommandé de traiter les valeurs manquantes en amont
val assembler = new VectorAssembler()
    .setInputCols(Array("feat1", "feat2"))
    .setOutputCol("features")
    .setHandleInvalid("skip")  // Option pour gérer les valeurs invalides
```

## Conversion vers formats exploitables

### Conversion en Array pour analyse
```scala
import org.apache.spark.ml.linalg.SparseVector
import org.apache.spark.sql.functions.udf

// UDF pour convertir Vector en Array
val toArray: Any => Array[Double] = _.asInstanceOf[SparseVector].toArray
val toArrayUdf = udf(toArray)

val dataWithArray = processedData
    .withColumn("features_array", toArrayUdf($"features"))

dataWithArray.show(truncate=false)
```

### Accès aux éléments individuels
```scala
import org.apache.spark.ml.linalg.Vector

// Fonction pour extraire un élément spécifique
def getVectorElement(vector: Vector, index: Int): Double = {
    vector(index)
}

// UDF pour extraire le premier élément
val getFirst = udf((v: Vector) => v(0))
val dataWithFirstFeature = processedData
    .withColumn("first_feature", getFirst($"features"))
```

## Exemple complet : du preprocessing au modèle

```scala
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

// Dataset exemple
case class Person(age: String, income: String, education: String, buys: String)
val data = Seq(
    Person("young", "high", "bachelor", "no"),
    Person("middle", "medium", "master", "yes"),
    Person("senior", "low", "phd", "no")
).toDF()

// Pipeline complet
val ageIndexer = new StringIndexer().setInputCol("age").setOutputCol("age_idx")
val incomeIndexer = new StringIndexer().setInputCol("income").setOutputCol("income_idx")
val eduIndexer = new StringIndexer().setInputCol("education").setOutputCol("edu_idx")
val labelIndexer = new StringIndexer().setInputCol("buys").setOutputCol("label")

val assembler = new VectorAssembler()
    .setInputCols(Array("age_idx", "income_idx", "edu_idx"))
    .setOutputCol("features")

val lr = new LogisticRegression()

val pipeline = new Pipeline().setStages(Array(
    ageIndexer, incomeIndexer, eduIndexer, labelIndexer, assembler, lr
))

// Entraînement et évaluation
val model = pipeline.fit(data)
val predictions = model.transform(data)

predictions.select("features", "label", "prediction").show()
```

## Avantages de VectorAssembler

### Performance
- **Optimisation mémoire** : Gestion intelligente des formats sparse/dense
- **Parallélisation** : Opérations vectorisées optimisées pour Spark
- **Lazy evaluation** : Transformations appliquées seulement lors du calcul

### Flexibilité
- **Types multiples** : Accepte colonnes numériques et vecteurs
- **Extensibilité** : Facile d'ajouter/supprimer des features
- **Pipeline integration** : S'intègre parfaitement dans les pipelines ML

### Robustesse
- **Gestion d'erreurs** : Options pour traiter les valeurs invalides
- **Type safety** : Vérifications de types à l'exécution
- **Metadata preservation** : Conserve les métadonnées des colonnes sources

## Limitations et considérations

### Contraintes techniques
- **Types supportés** : Uniquement Double et Vector
- **Ordre fixe** : L'ordre des features est déterminé par setInputCols
- **Transformation statique** : Structure du vecteur fixée à la compilation

### Bonnes pratiques
1. **Normalisation préalable** : Appliquer StandardScaler après VectorAssembler si nécessaire
2. **Feature selection** : Utiliser VectorSlicer pour sélectionner des sous-ensembles
3. **Documentation** : Maintenir un mapping entre indices et noms de features
4. **Validation** : Vérifier la cohérence des dimensions après transformation

## Debugging et validation

### Inspection des résultats
```scala
// Vérifier les dimensions
val assembledData = assembler.transform(data)
assembledData.select("features").first().getAs[Vector]("features").size

// Examiner le contenu
assembledData.select("features").show(truncate=false)

// Statistiques sur les vecteurs
import org.apache.spark.ml.stat.Summarizer
assembledData.select(Summarizer.metrics("mean", "std").summary($"features")).show()
```

### Validation des types
```scala
// Vérifier que toutes les colonnes d'entrée sont du bon type
val inputCols = Array("feat1", "feat2", "feat3")
inputCols.foreach { col =>
    val colType = data.schema(col).dataType
    println(s"$col: $colType")
}
```

## Conclusion

VectorAssembler est un composant essentiel des pipelines Spark ML qui transforme des données tabulaires en format vectoriel requis par les algorithmes de machine learning. Sa capacité à gérer efficacement différents types de données et formats de vecteurs en fait un outil indispensable pour la préparation des données en environnement distribué.

L'utilisation appropriée de VectorAssembler, combinée avec d'autres transformateurs comme StringIndexer et OneHotEncoder, permet de construire des pipelines de preprocessing robustes et performants pour des