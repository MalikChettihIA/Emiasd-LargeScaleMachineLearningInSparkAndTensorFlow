# VectorIndexer dans Spark ML - Guide Complet

## Vue d'ensemble

**VectorIndexer** est un transformateur intelligent de Spark ML qui analyse automatiquement un vecteur de features pour déterminer quelles dimensions sont catégorielles et lesquelles sont continues. Il indexe ensuite les features catégorielles tout en préservant les features continues.

## Analyse du code

```scala
import org.apache.spark.ml.feature.VectorIndexer

val vecIndexer = new VectorIndexer()
  .setInputCol("ageIncomeVec")
  .setOutputCol("ageIncomeVecInd")
  .setMaxCategories(3)

val vecIndexerModel = vecIndexer.fit(ageIncomeIndexedVec)
val categoricalFeatures: Set[Int] = vecIndexerModel.categoryMaps.keys.toSet
println(s"Chose ${categoricalFeatures.size} " +
  s"categorical features: ${categoricalFeatures.mkString(", ")}")

val ageIncomeIndexedVecInd = vecIndexerModel.transform(ageIncomeIndexedVec)
ageIncomeIndexedVecInd.select("ageIncomeVec","ageIncomeVecInd").show()
```

## Fonctionnement détaillé

### 1. Principe de classification automatique

VectorIndexer utilise un critère simple mais efficace :
- **Si nombre de valeurs distinctes ≤ MaxCategories** → Feature catégorielle
- **Sinon** → Feature continue

```scala
val vecIndexer = new VectorIndexer()
  .setInputCol("ageIncomeVec")        // Vecteur d'entrée
  .setOutputCol("ageIncomeVecInd")    // Vecteur de sortie
  .setMaxCategories(3)                // Seuil de décision
```

### 2. Processus de classification

**Exemple avec des données concrètes :**
```scala
// Vecteur d'entrée : [indexed_age, indexed_income]
// indexed_age peut être : 0.0, 1.0, 2.0 (3 valeurs distinctes)
// indexed_income peut être : 0.0, 1.0, 2.0 (3 valeurs distinctes)

// Avec setMaxCategories(3) :
// - indexed_age : 3 valeurs ≤ 3 → CATEGORIELLE
// - indexed_income : 3 valeurs ≤ 3 → CATEGORIELLE
```

### 3. Extraction des métadonnées

```scala
val categoricalFeatures: Set[Int] = vecIndexerModel.categoryMaps.keys.toSet
println(s"Chose ${categoricalFeatures.size} " +
  s"categorical features: ${categoricalFeatures.mkString(", ")}")
```

**categoryMaps** contient le mapping des valeurs pour chaque feature catégorielle :
- Clé : Index de la feature dans le vecteur (0, 1, 2...)
- Valeur : Map des valeurs originales vers leurs nouveaux indices

## Exemple pratique complet

### Données d'entrée après VectorAssembler

```scala
// Après StringIndexer + VectorAssembler
+-------------+
|ageIncomeVec |
+-------------+
|[1.0, 1.0]   |  // young, medium
|[1.0, 1.0]   |  // young, medium  
|[2.0, 1.0]   |  // middle, medium
|[0.0, 0.0]   |  // senior, high
|[0.0, 2.0]   |  // senior, low
|[0.0, 2.0]   |  // senior, low
|[2.0, 2.0]   |  // middle, low
|[1.0, 0.0]   |  // young, high
|[1.0, 2.0]   |  // young, low
|[0.0, 0.0]   |  // senior, high
+-------------+
```

### Analyse par VectorIndexer

```scala
// Feature 0 (age) : valeurs distinctes = {0.0, 1.0, 2.0} = 3 valeurs ≤ 3 → CATEGORIELLE
// Feature 1 (income) : valeurs distinctes = {0.0, 1.0, 2.0} = 3 valeurs ≤ 3 → CATEGORIELLE

// Résultat :
categoricalFeatures = Set(0, 1)
println("Chose 2 categorical features: 0, 1")
```

### Transformation appliquée

```scala
+-------------+----------------+
|ageIncomeVec |ageIncomeVecInd |
+-------------+----------------+
|[1.0, 1.0]   |[1.0, 1.0]      |  // Pas de changement (déjà indexées)
|[2.0, 1.0]   |[2.0, 1.0]      |  // Pas de changement
|[0.0, 0.0]   |[0.0, 0.0]      |  // Pas de changement
|[0.0, 2.0]   |[0.0, 2.0]      |  // Pas de changement
+-------------+----------------+
```

**Note importante** : Dans cet exemple, les valeurs ne changent pas car elles sont déjà correctement indexées par StringIndexer.

## Exemple avec données mixtes (catégorielles + continues)

### Dataset avec features mixtes

```scala
import org.apache.spark.ml.linalg.Vectors

val mixedData = Seq(
    Vectors.dense(1.0, 1.0, 18.5),  // [catégorie1, catégorie2, âge_réel]
    Vectors.dense(0.0, 2.0, 25.3),  // 
    Vectors.dense(1.0, 0.0, 31.7),  // 
    Vectors.dense(2.0, 3.0, 42.1)   // 
).map(v => (v))

val df = spark.createDataFrame(mixedData).toDF("features")
```

### Application de VectorIndexer

```scala
val vecIndexer = new VectorIndexer()
  .setInputCol("features")
  .setOutputCol("indexed_features")
  .setMaxCategories(3)

val model = vecIndexer.fit(df)
val categoricalFeatures = model.categoryMaps.keys.toSet
println(s"Categorical features: ${categoricalFeatures.mkString(", ")}")

// Résultat attendu :
// Feature 0: valeurs {0.0, 1.0, 2.0} = 3 valeurs ≤ 3 → CATEGORIELLE
// Feature 1: valeurs {0.0, 1.0, 2.0, 3.0} = 4 valeurs > 3 → CONTINUE  
// Feature 2: valeurs {18.5, 25.3, 31.7, 42.1} = 4 valeurs > 3 → CONTINUE

// Output: "Categorical features: 0"
```

### Résultat de la transformation

```scala
+------------------+------------------+
|features          |indexed_features  |
+------------------+------------------+
|[1.0,1.0,18.5]    |[1.0,1.0,18.5]   |  // Feature 0 indexée, autres inchangées
|[0.0,2.0,25.3]    |[0.0,2.0,25.3]   |  
|[1.0,0.0,31.7]    |[1.0,0.0,31.7]   |  
|[2.0,3.0,42.1]    |[2.0,3.0,42.1]   |  
+------------------+------------------+
```

## Cas d'usage avec réindexation nécessaire

### Quand VectorIndexer réindexe-t-il ?

VectorIndexer réindexe quand les valeurs catégorielles ne suivent pas l'ordre 0, 1, 2, 3...

```scala
// Exemple avec des valeurs non-ordonnées
val unorderedData = Seq(
    Vectors.dense(5.0, 10.0),   // Valeurs non-standard
    Vectors.dense(5.0, 20.0),   
    Vectors.dense(7.0, 10.0),   
    Vectors.dense(9.0, 20.0)    
).map(v => (v))

val df2 = spark.createDataFrame(unorderedData).toDF("features")

val vecIndexer2 = new VectorIndexer()
  .setInputCol("features")
  .setOutputCol("indexed_features")
  .setMaxCategories(4)

val model2 = vecIndexer2.fit(df2)
val result2 = model2.transform(df2)
```

### Mapping des valeurs

```scala
// VectorIndexer va créer un mapping comme :
// Feature 0: {5.0 → 0.0, 7.0 → 1.0, 9.0 → 2.0}
// Feature 1: {10.0 → 0.0, 20.0 → 1.0}

+----------+----------------+
|features  |indexed_features|
+----------+----------------+
|[5.0,10.0]|[0.0,0.0]      |  // 5.0→0.0, 10.0→0.0
|[5.0,20.0]|[0.0,1.0]      |  // 5.0→0.0, 20.0→1.0
|[7.0,10.0]|[1.0,0.0]      |  // 7.0→1.0, 10.0→0.0
|[9.0,20.0]|[2.0,1.0]      |  // 9.0→2.0, 20.0→1.0
+----------+----------------+
```

## Intégration dans un pipeline complet

### Pipeline de préparation des données

```scala
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler, VectorIndexer}
import org.apache.spark.ml.classification.DecisionTreeClassifier

// 1. Indexation des variables catégorielles individuelles
val ageIndexer = new StringIndexer()
    .setInputCol("age")
    .setOutputCol("indexed_age")

val incomeIndexer = new StringIndexer()
    .setInputCol("income") 
    .setOutputCol("indexed_income")

val labelIndexer = new StringIndexer()
    .setInputCol("buys_computer")
    .setOutputCol("label")

// 2. Assemblage en vecteur
val vectorAssembler = new VectorAssembler()
    .setInputCols(Array("indexed_age", "indexed_income"))
    .setOutputCol("featuresVec")

// 3. Indexation intelligente du vecteur
val vectorIndexer = new VectorIndexer()
    .setInputCol("featuresVec")
    .setOutputCol("features")
    .setMaxCategories(3)

// 4. Algorithme de classification
val dt = new DecisionTreeClassifier()

// 5. Pipeline complet
val pipeline = new Pipeline()
    .setStages(Array(ageIndexer, incomeIndexer, labelIndexer, 
                    vectorAssembler, vectorIndexer, dt))

val model = pipeline.fit(trainingData)
```

### Pourquoi cette étape est-elle nécessaire ?

**Pour les arbres de décision** :
- Les features catégorielles utilisent des conditions d'égalité (`feature = 1`)
- Les features continues utilisent des conditions d'inégalité (`feature ≤ 0.5`)
- VectorIndexer informe l'algorithme sur le type de chaque feature

## Analyse des métadonnées

### Inspection du modèle VectorIndexer

```scala
val vecIndexerModel = vecIndexer.fit(data)

// 1. Features catégorielles identifiées
val categoricalFeatures = vecIndexerModel.categoryMaps.keys.toSet
println(s"Categorical features: ${categoricalFeatures.mkString(", ")}")

// 2. Mapping détaillé pour chaque feature catégorielle
vecIndexerModel.categoryMaps.foreach { case (featureIndex, mapping) =>
    println(s"Feature $featureIndex mapping:")
    mapping.foreach { case (originalValue, newIndex) =>
        println(s"  $originalValue → $newIndex")
    }
}

// 3. Nombre total de features
println(s"Total features: ${vecIndexerModel.numFeatures}")
```

### Exemple de sortie

```scala
// Sortie typique :
Categorical features: 0, 1
Feature 0 mapping:
  0.0 → 0.0
  1.0 → 1.0  
  2.0 → 2.0
Feature 1 mapping:
  0.0 → 0.0
  1.0 → 1.0
  2.0 → 2.0
Total features: 2
```

## Optimisation et bonnes pratiques

### Choix du paramètre MaxCategories

```scala
// Trop bas : features catégorielles traitées comme continues
val restrictiveIndexer = new VectorIndexer()
    .setMaxCategories(2)  // Seules les features binaires sont catégorielles

// Trop haut : features continues traitées comme catégorielles  
val permissiveIndexer = new VectorIndexer()
    .setMaxCategories(100)  // Risque de sur-catégorisation

// Recommandé : basé sur la connaissance du domaine
val balancedIndexer = new VectorIndexer()
    .setMaxCategories(10)  // Compromis raisonnable
```

### Validation des résultats

```scala
// Vérifier la classification automatique
val model = vecIndexer.fit(data)
val categoricalFeatures = model.categoryMaps.keys.toSet
val totalFeatures = model.numFeatures

println(s"Features catégorielles : ${categoricalFeatures.size}/$totalFeatures")
println(s"Features continues : ${totalFeatures - categoricalFeatures.size}/$totalFeatures")

// Valider manuellement si nécessaire
data.select("features").take(5).foreach { row =>
    val vector = row.getAs[org.apache.spark.ml.linalg.Vector]("features")
    println(s"Sample vector: ${vector.toArray.mkString(", ")}")
}
```

## Cas d'usage avancés

### Handling de données sparse

```scala
import org.apache.spark.ml.linalg.Vectors

// Données avec vecteurs sparse
val sparseData = Seq(
    Vectors.sparse(5, Array(0, 2), Array(1.0, 3.0)),  // Position 0 et 2 non-nulles
    Vectors.sparse(5, Array(1, 4), Array(2.0, 1.0)),  // Position 1 et 4 non-nulles
    Vectors.sparse(5, Array(0, 3), Array(1.0, 2.0))   // Position 0 et 3 non-nulles
).map(v => (v))

val sparseDF = spark.createDataFrame(sparseData).toDF("sparse_features")

val sparseIndexer = new VectorIndexer()
    .setInputCol("sparse_features")
    .setOutputCol("indexed_sparse")
    .setMaxCategories(4)

// VectorIndexer gère automatiquement les vecteurs sparse
val sparseModel = sparseIndexer.fit(sparseDF)
val sparseResult = sparseModel.transform(sparseDF)
```

### Préservation des métadonnées

```scala
// VectorIndexer préserve et enrichit les métadonnées
val originalSchema = data.schema
val indexedData = vecIndexerModel.transform(data)
val newSchema = indexedData.schema

// Comparer les schémas
println("Schema original:")
originalSchema.fields.foreach(field => println(s"  ${field.name}: ${field.dataType}"))

println("Schema après indexation:")
newSchema.fields.foreach(field => println(s"  ${field.name}: ${field.dataType}"))
```

## Debugging et troubleshooting

### Problèmes courants

1. **MaxCategories trop restrictif**
```scala
// Symptôme : features catégorielles traitées comme continues
// Solution : augmenter MaxCategories
val fixedIndexer = vecIndexer.setMaxCategories(20)
```

2. **Données déjà correctement indexées**
```scala
// VectorIndexer peut sembler "ne rien faire"
// C'est normal si les données sont déjà au bon format
```

3. **Vecteurs de tailles différentes**
```scala
// VectorIndexer nécessite des vecteurs de taille uniforme
// Utiliser VectorAssembler au préalable si nécessaire
```

### Inspection détaillée

```scala
// Debug : analyser les valeurs distinctes par feature
import org.apache.spark.ml.linalg.Vector

val analyzeFeatures = (vectors: Array[Vector]) => {
    val numFeatures = vectors.head.size
    for (i <- 0 until numFeatures) {
        val distinctValues = vectors.map(_.apply(i)).distinct
        println(s"Feature $i: ${distinctValues.length} distinct values: ${distinctValues.mkString(", ")}")
    }
}

val vectorArray = data.select("features").collect().map(_.getAs[Vector]("features"))
analyzeFeatures(vectorArray)
```

## Performance et scalabilité

### Optimisations

```scala
// 1. Cache des données si utilisées plusieurs fois
val cachedData = data.cache()
val model1 = vecIndexer1.fit(cachedData)
val model2 = vecIndexer2.fit(cachedData)  // Réutilise le cache

// 2. Parallélisation automatique
// VectorIndexer tire parti de la parallélisation Spark automatiquement

// 3. Gestion mémoire pour gros datasets
val efficientIndexer = new VectorIndexer()
    .setMaxCategories(50)  // Limiter la complexité
    .setInputCol("features")
    .setOutputCol("indexed_features")
```

## Intégration avec les algorithmes ML

### Decision Trees

```scala
// VectorIndexer est particulièrement important pour les arbres
val dt = new DecisionTreeClassifier()
    .setFeaturesCol("indexed_features")  // Utilise les métadonnées de VectorIndexer
    .setLabelCol("label")

val dtModel = dt.fit(indexedData)

// L'arbre utilise les bonnes conditions selon le type de feature
println(dtModel.toDebugString)
```

### Random Forest

```scala
import org.apache.spark.ml.classification.RandomForestClassifier

val rf = new RandomForestClassifier()
    .setFeaturesCol("indexed_features")
    .setLabelCol("label")
    .setNumTrees(10)

// Random Forest bénéficie aussi de l'information catégorielle/continue
val rfModel = rf.fit(indexedData)
```

## Conclusion

VectorIndexer est un transformateur intelligent qui automatise la distinction entre features catégorielles et continues dans un vecteur. Il est particulièrement crucial pour les algorithmes basés sur les arbres (Decision Tree, Random Forest) qui nécessitent cette information pour générer les bonnes conditions de division.

### Points clés à retenir

- **Classification automatique** basée sur le nombre de valeurs distinctes
- **Paramètre MaxCategories** détermine le seuil de classification
- **Métadonnées enrichies** informent les algorithmes sur le type des features
- **Intégration pipeline** standard dans les workflows de preprocessing
- **Gestion automatique** des formats sparse et dense
- **Performance optimisée** pour le traitement distribué

VectorIndexer représente un maillon essentiel entre la préparation des données (VectorAssembler) et l'entraînement des modèles, automatisant une décision critique qui impacte directement la qualité des modèles de machine learning.