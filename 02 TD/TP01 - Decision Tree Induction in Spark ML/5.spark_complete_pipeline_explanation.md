# Pipeline Complet de Preprocessing Spark ML - Analyse Détaillée

## Vue d'ensemble

Ce code implémente un **pipeline complet de preprocessing** pour préparer des données à l'entraînement d'un arbre de décision dans Spark ML. Il automatise toute la chaîne de transformation : indexation des variables catégorielles, assemblage en vecteur, et indexation vectorielle.

## Analyse section par section

### 1. Indexation du label (variable cible)

```scala
import org.apache.spark.ml.feature.StringIndexer

/*index the label attribute*/
val labName = "buys_computer"
val stringIndexerLabel = new StringIndexer()
    .setInputCol(labName)
    .setOutputCol("label") 
```

**Objectif** : Convertir la variable cible catégorielle en indices numériques.

**Transformation** :
- `"yes"` → `0.0` (plus fréquent)
- `"no"` → `1.0` (moins fréquent)

**Pourquoi "label" ?** : Convention Spark ML - les algorithmes attendent une colonne nommée "label".

### 2. Identification et indexation des features

```scala
val inFeatureAtts = data.columns.filterNot(_.contains(labName))
val outFeatureAtts = inFeatureAtts.map("indexed_"+_)
val stringIndexerFeatures = new StringIndexer().setInputCols(inFeatureAtts).setOutputCols(outFeatureAtts)
```

**Décomposition** :

1. **`inFeatureAtts`** : Récupère toutes les colonnes sauf la cible
   ```scala
   // Si data.columns = ["age", "income", "student", "credit_rating", "buys_computer"]
   // inFeatureAtts = ["age", "income", "student", "credit_rating"]
   ```

2. **`outFeatureAtts`** : Génère les noms de colonnes de sortie
   ```scala
   // outFeatureAtts = ["indexed_age", "indexed_income", "indexed_student", "indexed_credit_rating"]
   ```

3. **`stringIndexerFeatures`** : Indexeur multiple pour toutes les features
   - **setInputCols** : Colonnes d'entrée (toutes les features)
   - **setOutputCols** : Colonnes de sortie (avec préfixe "indexed_")

### 3. Enregistrement des champs indexés

```scala
/*register the indexed fields*/
val indexedFields = stringIndexerFeatures.getOutputCols
```

**Résultat** : `Array("indexed_age", "indexed_income", "indexed_student", "indexed_credit_rating")`

### 4. Création du mapping feature → indice

```scala
/*create a map to register the correspondance between attribute names and their feature indice.
Eg. age will be feature 0 etc 
This will be useful to read the inferred decision tree*/
val featureIndices = indexedFields.zipWithIndex.map{case(strInd,ind)=>("feature "+ind,strInd)}.toMap
```

**Décomposition** :
- **zipWithIndex** : Associe chaque nom avec son indice
- **map** : Transforme en paires ("feature X", "nom_original")

**Résultat** :
```scala
Map(
  "feature 0" -> "indexed_age",
  "feature 1" -> "indexed_income", 
  "feature 2" -> "indexed_student",
  "feature 3" -> "indexed_credit_rating"
)
```

**Utilité** : Interpréter l'arbre de décision qui utilise des indices ("feature 0", "feature 1", etc.)

### 5. Assemblage vectoriel

```scala
import org.apache.spark.ml.feature.VectorAssembler

/*create a vector of features from all the indexed attributes 
except the target column label */
val vectorAssembler = new VectorAssembler()
                    .setInputCols(indexedFields)
                    .setOutputCol("featuresVec")
```

**Transformation** :
```scala
// De :
indexed_age: 1.0, indexed_income: 0.0, indexed_student: 1.0, indexed_credit_rating: 0.0

// Vers :
featuresVec: [1.0, 0.0, 1.0, 0.0]
```

### 6. Indexation vectorielle intelligente

```scala
import org.apache.spark.ml.feature.VectorIndexer

/* index the vector of features to account for categorical features*/
val maxCat = 3
val vectorIndexer = new VectorIndexer()
  .setInputCol("featuresVec")
  .setOutputCol("features")
  .setMaxCategories(maxCat)
```

**Rôle** : Identifie automatiquement les features catégorielles vs continues dans le vecteur.

**Logique** : Si nombre de valeurs distinctes ≤ 3 → catégorielle, sinon → continue.

### 7. Construction du pipeline

```scala
/*chain the tranformations in one pipeline*/
import  org.apache.spark.ml.Pipeline 
val pipeline = new Pipeline()
                    .setStages(Array(stringIndexerLabel,stringIndexerFeatures,vectorAssembler,vectorIndexer))
```

**Étapes du pipeline** :
1. **stringIndexerLabel** : Indexe la variable cible
2. **stringIndexerFeatures** : Indexe toutes les features
3. **vectorAssembler** : Combine les features en vecteur  
4. **vectorIndexer** : Identifie les types de features

### 8. Entraînement et application

```scala
val train_data_model = pipeline.fit(data)

train_data_model.stages.foreach(println)

val train_data_allatts = train_data_model.transform(data)
val train_data = train_data_allatts.select("features","label")

train_data.show()
```

**Processus** :
1. **fit(data)** : Entraîne tous les transformateurs sur les données
2. **stages.foreach(println)** : Affiche les étapes du pipeline entraîné
3. **transform(data)** : Applique toutes les transformations
4. **select("features","label")** : Garde uniquement les colonnes finales

## Exemple de transformation complète

### Données d'origine
```scala
+------+------+-------+-------------+-------------+
|   age|income|student|credit_rating|buys_computer|
+------+------+-------+-------------+-------------+
| young|  high|     no|         fair|           no|
|middle|  high|     no|         fair|          yes|
|senior|medium|     no|         fair|          yes|
|senior|   low|    yes|         fair|          yes|
```

### Après stringIndexerLabel
```scala
+------+------+-------+-------------+-------------+-----+
|   age|income|student|credit_rating|buys_computer|label|
+------+------+-------+-------------+-------------+-----+
| young|  high|     no|         fair|           no|  1.0|
|middle|  high|     no|         fair|          yes|  0.0|
|senior|medium|     no|         fair|          yes|  0.0|
|senior|   low|    yes|         fair|          yes|  0.0|
```

### Après stringIndexerFeatures
```scala
+------+------+-------+-------------+----------+-------------+-------------+-------------------+
|   age|income|student|credit_rating|indexed_age|indexed_income|indexed_student|indexed_credit_rating|
+------+------+-------+-------------+----------+-------------+-------------+-------------------+
| young|  high|     no|         fair|       1.0|          0.0|          0.0|                0.0|
|middle|  high|     no|         fair|       2.0|          0.0|          0.0|                0.0|
|senior|medium|     no|         fair|       0.0|          1.0|          0.0|                0.0|
|senior|   low|    yes|         fair|       0.0|          2.0|          1.0|                0.0|
```

### Après vectorAssembler
```scala
+----------+-------------+-------------+-------------------+-------------------+
|indexed_age|indexed_income|indexed_student|indexed_credit_rating|         featuresVec|
+----------+-------------+-------------+-------------------+-------------------+
|       1.0|          0.0|          0.0|                0.0|  [1.0,0.0,0.0,0.0]|
|       2.0|          0.0|          0.0|                0.0|  [2.0,0.0,0.0,0.0]|
|       0.0|          1.0|          0.0|                0.0|  [0.0,1.0,0.0,0.0]|
|       0.0|          2.0|          1.0|                0.0|  [0.0,2.0,1.0,0.0]|
```

### Après vectorIndexer (résultat final)
```scala
+-------------------+-----+
|           features|label|
+-------------------+-----+
|  [1.0,0.0,0.0,0.0]|  1.0|
|  [2.0,0.0,0.0,0.0]|  0.0|
|  [0.0,1.0,0.0,0.0]|  0.0|
|  [0.0,2.0,1.0,0.0]|  0.0|
```

## Architecture du pipeline

### Diagramme conceptuel
```
Données brutes
      ↓
[StringIndexer Label] → Indexe "buys_computer" → "label"
      ↓
[StringIndexer Features] → Indexe toutes les features → "indexed_*"
      ↓  
[VectorAssembler] → Combine en vecteur → "featuresVec"
      ↓
[VectorIndexer] → Identifie types → "features"
      ↓
Données prêtes pour ML
```

### Types de transformateurs

1. **Estimator** : StringIndexer (nécessite fit())
2. **Transformer** : VectorAssembler (transformation directe)
3. **Estimator** : VectorIndexer (analyse les types)

## Avantages du pipeline

### 1. Reproductibilité
```scala
// Le pipeline peut être sauvegardé et réutilisé
pipeline.write.overwrite().save("path/to/pipeline")

// Et rechargé plus tard
val loadedPipeline = Pipeline.load("path/to/pipeline")
```

### 2. Cohérence train/test
```scala
// Même preprocessing pour train et test
val trainTransformed = trainedPipeline.transform(trainData)
val testTransformed = trainedPipeline.transform(testData)
```

### 3. Simplicité d'usage
```scala
// Une seule ligne pour tout le preprocessing
val processedData = pipeline.fit(rawData).transform(rawData)
```

## Utilisation pour l'arbre de décision

### Entraînement du modèle
```scala
import org.apache.spark.ml.classification.DecisionTreeClassifier

val dt = new DecisionTreeClassifier()
  .setLabelCol("label")
  .setFeaturesCol("features")

val dtModel = dt.fit(train_data)
println(s"Learned classification tree model:\n ${dtModel.toDebugString}")
```

### Interprétation avec featureIndices
```scala
// L'arbre affiche "feature 0", "feature 1", etc.
// Utiliser featureIndices pour retrouver les noms originaux
featureIndices.foreach { case (featureIndex, originalName) =>
  println(s"$featureIndex corresponds to $originalName")
}

// Sortie :
// feature 0 corresponds to indexed_age
// feature 1 corresponds to indexed_income
// feature 2 corresponds to indexed_student  
// feature 3 corresponds to indexed_credit_rating
```

## Bonnes pratiques et extensions

### 1. Gestion des valeurs manquantes
```scala
val robustStringIndexer = new StringIndexer()
  .setInputCols(inFeatureAtts)
  .setOutputCols(outFeatureAtts)
  .setHandleInvalid("skip")  // ou "keep", "error"
```

### 2. Pipeline étendu avec validation
```scala
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}

val fullPipeline = new Pipeline()
  .setStages(Array(stringIndexerLabel, stringIndexerFeatures, 
                  vectorAssembler, vectorIndexer, dt))

val paramGrid = new ParamGridBuilder()
  .addGrid(dt.maxDepth, Array(3, 5, 7))
  .build()

val cv = new CrossValidator()
  .setEstimator(fullPipeline)
  .setEvaluator(new MulticlassClassificationEvaluator())
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(3)

val cvModel = cv.fit(data)
```

### 3. Debugging et inspection
```scala
// Inspecter chaque étape
val pipelineModel = pipeline.fit(data)
pipelineModel.stages.zipWithIndex.foreach { case (stage, index) =>
  println(s"Stage $index: ${stage.getClass.getSimpleName}")
  
  // Pour les StringIndexer, afficher les mappings
  if (stage.isInstanceOf[StringIndexerModel]) {
    val stringIndexerModel = stage.asInstanceOf[StringIndexerModel]
    println(s"  Labels: ${stringIndexerModel.labels.mkString(", ")}")
  }
}
```

## Optimisations et performance

### 1. Cache stratégique
```scala
// Cache après les transformations coûteuses
val indexedData = stringIndexers.fit(data).transform(data).cache()
val finalData = vectorTransformers.transform(indexedData)
```

### 2. Parallelisation
```scala
// Spark parallélise automatiquement les transformations
// Optimiser le nombre de partitions si nécessaire
val repartitionedData = data.repartition(200)
val processedData = pipeline.fit(repartitionedData).transform(repartitionedData)
```

## Conclusion

Ce pipeline illustre parfaitement les bonnes pratiques de preprocessing dans Spark ML :

- **Séparation claire** entre label et features
- **Transformations chainées** dans un ordre logique
- **Mapping documenté** pour l'interprétabilité
- **Format standardisé** pour les algorithmes ML
- **Reproductibilité** garantie par le pipeline

Il constitue un template robuste pour tout projet de classification avec des variables catégorielles, automatisant les étapes fastidieuses tout en conservant la traçabilité nécessaire à l'interprétation des modèles.