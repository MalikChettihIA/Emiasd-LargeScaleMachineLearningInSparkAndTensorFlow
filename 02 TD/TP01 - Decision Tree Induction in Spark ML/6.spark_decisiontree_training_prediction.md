# Entraînement et Prédiction avec DecisionTreeClassifier - Guide Complet

## Vue d'ensemble

Ce code illustre le cycle complet d'un projet de machine learning : **entraînement d'un arbre de décision** sur des données preprocessées, puis **prédiction sur de nouvelles données** en appliquant le même preprocessing.

## Analyse section par section

### 1. Entraînement de l'arbre de décision

```scala
import org.apache.spark.ml.classification.DecisionTreeClassificationModel
import org.apache.spark.ml.classification.DecisionTreeClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

val dt = new DecisionTreeClassifier()
//   .setLabelCol("label")
//   .setFeaturesCol("features")

val dtModel = dt.fit(train_data)
println(s"Learned classification tree model:\n ${dtModel.toDebugString}")

display(dtModel)
```

#### Imports nécessaires
- **DecisionTreeClassifier** : L'algorithme d'entraînement
- **DecisionTreeClassificationModel** : Le modèle entraîné
- **MulticlassClassificationEvaluator** : Pour évaluer les performances

#### Configuration de l'algorithme
```scala
val dt = new DecisionTreeClassifier()
```

**Paramètres par défaut** :
- `labelCol = "label"` (commenté car c'est la valeur par défaut)
- `featuresCol = "features"` (commenté car c'est la valeur par défaut)
- `impurity = "gini"` (mesure de pureté)
- `maxDepth = 5` (profondeur maximale)

#### Entraînement du modèle
```scala
val dtModel = dt.fit(train_data)
```

**Processus d'entraînement** :
1. Analyse les features catégorielles vs continues (grâce à VectorIndexer)
2. Construit l'arbre en minimisant l'impureté
3. Retourne un modèle prêt pour la prédiction

#### Visualisation de l'arbre
```scala
println(s"Learned classification tree model:\n ${dtModel.toDebugString}")
display(dtModel)
```

**Sortie typique** :
```
DecisionTreeClassificationModel: uid=dtc_123, depth=3, numNodes=7
  If (feature 0 <= 0.5)
   If (feature 1 <= 1.5)
    Predict: 1.0
   Else (feature 1 > 1.5)
    Predict: 0.0
  Else (feature 0 > 0.5)
   Predict: 1.0
```

### 2. Création de données de test

```scala
import spark.implicits._
val test_df = Seq(("young","high","no","fair"),
                (("senior","high","yes","excellent"))
                 )
            .toDF("age","income","student","credit_rating")
test_df.show()
```

**Données de test créées** :
```
+------+------+-------+-------------+
|   age|income|student|credit_rating|
+------+------+-------+-------------+
| young|  high|     no|         fair|
|senior|  high|    yes|    excellent|
+------+------+-------+-------------+
```

**Points importants** :
- **Même schéma** que les données d'entraînement (sans la colonne cible)
- **Valeurs cohérentes** avec le vocabulaire d'entraînement
- **Pas de colonne "buys_computer"** (on veut la prédire)

### 3. Pipeline de prédiction intelligent

```scala
val predictionPipeline = new Pipeline().setStages(pipeline.getStages.slice(1,pipeline.getStages.size))

pipeline.getStages.slice(1,pipeline.getStages.size).foreach(println)
```

#### Logique du slicing
**Pipeline original** :
```
[StringIndexerLabel, StringIndexerFeatures, VectorAssembler, VectorIndexer]
```

**Pipeline de prédiction** :
```scala
.slice(1, pipeline.getStages.size)
// Résultat : [StringIndexerFeatures, VectorAssembler, VectorIndexer]
```

**Pourquoi exclure StringIndexerLabel ?**
- Les données de test n'ont **pas de colonne cible**
- StringIndexerLabel nécessite la colonne "buys_computer" qui n'existe pas
- On garde seulement les transformations des features

#### Étapes conservées
```scala
// Sortie de pipeline.getStages.slice(1,pipeline.getStages.size).foreach(println)
StringIndexerModel: uid=strIdx_456
VectorAssembler: uid=vecAssembler_789  
VectorIndexerModel: uid=vecIdx_012
```

### 4. Application du preprocessing aux données de test

```scala
val test_data = predictionPipeline.fit(test_df).transform(test_df).select("features")
test_data.show()
```

#### Processus de transformation
1. **fit(test_df)** : Ajuste les transformateurs sur les nouvelles données
2. **transform(test_df)** : Applique les transformations
3. **select("features")** : Conserve seulement la colonne vecteur final

#### Résultat attendu
```scala
+-------------------+
|           features|
+-------------------+
|  [1.0,0.0,0.0,0.0]|  // young, high, no, fair
|  [0.0,0.0,1.0,1.0]|  // senior, high, yes, excellent  
+-------------------+
```

**Mapping** (basé sur le featureIndices du pipeline d'entraînement) :
- **Feature 0** : age → young=1.0, senior=0.0
- **Feature 1** : income → high=0.0
- **Feature 2** : student → no=0.0, yes=1.0
- **Feature 3** : credit_rating → fair=0.0, excellent=1.0

### 5. Prédiction avec le modèle entraîné

```scala
val predictions = dtModel.transform(test_data)

// Select example rows to display.
predictions.show(false)
```

#### Résultat de la prédiction
```scala
+-------------------+----------+--------------------+
|           features|prediction|         probability|
+-------------------+----------+--------------------+
|  [1.0,0.0,0.0,0.0]|       1.0|   [0.2,0.8]       |  // Predict: no (80%)
|  [0.0,0.0,1.0,1.0]|       0.0|   [0.9,0.1]       |  // Predict: yes (90%)
+-------------------+----------+--------------------+
```

**Interprétation** :
- **prediction = 1.0** → "no" (n'achète pas)
- **prediction = 0.0** → "yes" (achète)
- **probability** → [P(class=0), P(class=1)]

## Workflow complet visualisé

### Architecture de prédiction

```
Nouvelles données brutes
         ↓
[StringIndexerFeatures] → Indexe age, income, student, credit_rating
         ↓
[VectorAssembler] → Combine en vecteur [1.0,0.0,0.0,0.0]
         ↓
[VectorIndexer] → Identifie types catégorielles/continues
         ↓
Vecteur "features" compatible
         ↓
[DecisionTreeModel] → Applique les règles apprises
         ↓
Prédictions finales
```

### Correspondance avec l'entraînement

| Étape | Entraînement | Prédiction |
|-------|-------------|------------|
| **Input** | Données avec label | Données sans label |
| **Preprocessing** | Pipeline complet | Pipeline sans StringIndexerLabel |
| **Output** | Modèle entraîné | Prédictions |

## Exemples d'amélioration du code

### 1. Pipeline de prédiction plus robuste

```scala
// Approche plus sûre : créer un pipeline dédié à la prédiction
val predictionStages = Array(
    stringIndexerFeatures,  // Réutilise le même indexeur
    vectorAssembler,
    vectorIndexer
)

val predictionPipeline = new Pipeline().setStages(predictionStages)

// Utiliser le modèle de preprocessing déjà entraîné
val preprocessingModel = pipeline.stages.slice(1, pipeline.stages.size)
val test_data = preprocessingModel.foldLeft(test_df)((df, stage) => 
    stage.asInstanceOf[Transformer].transform(df)
).select("features")
```

### 2. Validation et gestion d'erreurs

```scala
// Vérifier la cohérence du schéma
val expectedColumns = Array("age", "income", "student", "credit_rating")
val actualColumns = test_df.columns

if (!expectedColumns.forall(actualColumns.contains)) {
    throw new IllegalArgumentException(s"Missing columns: ${expectedColumns.diff(actualColumns).mkString(", ")}")
}

// Vérifier les valeurs inconnues
val knownAgeValues = Array("young", "middle", "senior")
val unknownAges = test_df.select("age").distinct().rdd
    .map(_.getString(0))
    .filter(!knownAgeValues.contains(_))
    .collect()

if (unknownAges.nonEmpty) {
    println(s"Warning: Unknown age values: ${unknownAges.mkString(", ")}")
}
```

### 3. Prédiction avec métriques de confiance

```scala
import org.apache.spark.sql.functions._

val predictionsWithConfidence = predictions
    .withColumn("max_probability", 
        greatest(col("probability").getItem(0), col("probability").getItem(1)))
    .withColumn("predicted_class", 
        when(col("prediction") === 0.0, "yes").otherwise("no"))
    .withColumn("confidence_level",
        when(col("max_probability") > 0.9, "high")
        .when(col("max_probability") > 0.7, "medium")
        .otherwise("low"))

predictionsWithConfidence.select("features", "predicted_class", 
                                "max_probability", "confidence_level").show()
```

## Analyse des résultats

### Interprétation business

```scala
// Exemple de résultats
// Personne 1: young, high income, not student, fair credit → Prediction: "no" (80% confiance)
// Personne 2: senior, high income, student, excellent credit → Prediction: "yes" (90% confiance)
```

**Insights possibles** :
1. **Jeunes à haut revenu** : Tendance à ne pas acheter (peut-être pas la cible)
2. **Seniors étudiants avec excellent crédit** : Forte probabilité d'achat
3. **Crédit rating** semble être un facteur discriminant important

### Validation du modèle

```scala
// Utiliser featureIndices pour comprendre les règles
println("Feature importance mapping:")
dtModel.featureImportances.toArray.zipWithIndex.foreach { case (importance, index) =>
    val featureName = featureIndices.getOrElse(s"feature $index", s"feature $index")
    println(s"$featureName: ${importance}")
}

// Exemple de sortie:
// indexed_age: 0.45
// indexed_income: 0.30  
// indexed_student: 0.15
// indexed_credit_rating: 0.10
```

## Gestion de la production

### 1. Sauvegarde du modèle

```scala
// Sauvegarder le pipeline complet de preprocessing
pipeline.write.overwrite().save("path/to/preprocessing_pipeline")

// Sauvegarder le modèle d'arbre de décision
dtModel.write.overwrite().save("path/to/decision_tree_model")
```

### 2. Chargement et utilisation

```scala
// Charger les modèles sauvegardés
val loadedPipeline = Pipeline.load("path/to/preprocessing_pipeline")
val loadedDTModel = DecisionTreeClassificationModel.load("path/to/decision_tree_model")

// Fonction de prédiction réutilisable
def predictNewData(rawData: DataFrame): DataFrame = {
    val predictionPipeline = new Pipeline()
        .setStages(loadedPipeline.getStages.slice(1, loadedPipeline.getStages.size))
    
    val preprocessed = predictionPipeline.fit(rawData).transform(rawData).select("features")
    loadedDTModel.transform(preprocessed)
}
```

### 3. API de prédiction

```scala
import org.apache.spark.sql.functions._

def makePrediction(age: String, income: String, student: String, creditRating: String): (String, Double) = {
    val inputData = Seq((age, income, student, creditRating))
        .toDF("age", "income", "student", "credit_rating")
    
    val prediction = predictNewData(inputData)
        .select("prediction", "probability")
        .collect()(0)
    
    val predictedClass = if (prediction.getDouble(0) == 0.0) "yes" else "no"
    val confidence = prediction.getAs[Vector](1).toArray.max
    
    (predictedClass, confidence)
}

// Utilisation
val (result, confidence) = makePrediction("young", "high", "no", "fair")
println(s"Prediction: $result with ${confidence*100}% confidence")
```

## Conclusion

Ce code illustre parfaitement le cycle complet d'un projet de machine learning :

1. **Entraînement** sur données preprocessées
2. **Création de pipeline de prédiction** (sans le label)
3. **Application cohérente** du preprocessing
4. **Prédiction** sur nouvelles données
5. **Interprétation** des résultats

La clé du succès réside dans la **cohérence du preprocessing** entre l'entraînement et la prédiction, garantie par la réutilisation intelligente du pipeline original.