# AutoPipeline et Analyse de Métadonnées - Guide Complet

## Vue d'ensemble

Ce code présente deux fonctions essentielles pour l'automatisation du preprocessing dans Spark ML :
1. **AutoPipeline** : Une fonction générique qui construit automatiquement un pipeline de preprocessing
2. **MDCompletenessDV** : Une fonction d'analyse de qualité des données qui extrait les métadonnées

## 1. Fonction AutoPipeline - Automatisation du Pipeline

### Signature et paramètres

```scala
def AutoPipeline(textCols: Array[String], numericCols: Array[String], target: String, maxCat: Int, handleInvalid: String):Pipeline
```

**Paramètres d'entrée** :
- **textCols** : Colonnes catégorielles à indexer
- **numericCols** : Colonnes numériques à conserver telles quelles
- **target** : Variable cible (label)
- **maxCat** : Seuil pour VectorIndexer (catégorielle vs continue)
- **handleInvalid** : Stratégie pour gérer les valeurs invalides ("skip", "error", "keep")

### Analyse détaillée du code

#### Étape 1 : Préparation des colonnes pour StringIndexer

```scala
val inAttsNames = textCols ++ Array(target)
val outAttsNames = inAttsNames.map(_prefix+_)
```

**Logique** :
- **inAttsNames** : Combine features textuelles + variable cible
- **outAttsNames** : Génère les noms de sortie avec préfixe "indexed_"

**Exemple** :
```scala
// Si textCols = ["age", "income"] et target = "buys_computer"
// inAttsNames = ["age", "income", "buys_computer"]
// outAttsNames = ["indexed_age", "indexed_income", "indexed_buys_computer"]
```

#### Étape 2 : Configuration du StringIndexer

```scala
val stringIndexer = new StringIndexer()
                            //to be completed
```

**Code complet attendu** :
```scala
val stringIndexer = new StringIndexer()
    .setInputCols(inAttsNames)
    .setOutputCols(outAttsNames)
    .setHandleInvalid(handleInvalid)
```

**Fonction** : Indexe toutes les colonnes catégorielles (features + target) en une seule opération.

#### Étape 3 : Préparation des features pour VectorAssembler

```scala
val features = outFeatureAtts.filterNot(_.contains(target))++numericCols
```

**Décomposition** :
- **outFeatureAtts.filterNot(_.contains(target))** : Features indexées (sans le label)
- **++numericCols** : Ajoute les colonnes numériques non transformées

**Exemple** :
```scala
// outFeatureAtts = ["indexed_age", "indexed_income", "indexed_buys_computer"]
// target = "buys_computer"
// numericCols = ["salary", "experience"]
// 
// features = ["indexed_age", "indexed_income", "salary", "experience"]
```

#### Étape 4 : Configuration du VectorAssembler

```scala
val vectorAssembler = new VectorAssembler()
                        //to be completed
```

**Code complet attendu** :
```scala
val vectorAssembler = new VectorAssembler()
    .setInputCols(features)
    .setOutputCol("featuresVec")
    .setHandleInvalid(handleInvalid)
```

#### Étape 5 : Configuration du VectorIndexer

```scala
val vectorIndexer = new VectorIndexer()
                        //to be completed
```

**Code complet attendu** :
```scala
val vectorIndexer = new VectorIndexer()
    .setInputCol("featuresVec")
    .setOutputCol("features")
    .setMaxCategories(maxCat)
    .setHandleInvalid(handleInvalid)
```

#### Étape 6 : Assemblage du pipeline

```scala
val pipeline = new Pipeline()
                .setStages(Array(stringIndexer,vectorAssembler,vectorIndexer))

return pipeline
```

### Version complète de AutoPipeline

```scala
def AutoPipeline(textCols: Array[String], numericCols: Array[String], target: String, maxCat: Int, handleInvalid: String):Pipeline = {
  // StringIndexer pour toutes les colonnes catégorielles + target
  val inAttsNames = textCols ++ Array(target)
  val outAttsNames = inAttsNames.map(_prefix+_)

  val stringIndexer = new StringIndexer()
                            .setInputCols(inAttsNames)
                            .setOutputCols(outAttsNames)
                            .setHandleInvalid(handleInvalid)
  
  // Features = colonnes indexées (sans target) + colonnes numériques
  val features = outAttsNames.filterNot(_.contains(target)) ++ numericCols
  
  // VectorAssembler pour combiner toutes les features
  val vectorAssembler = new VectorAssembler()
                          .setInputCols(features)
                          .setOutputCol("featuresVec")
                          .setHandleInvalid(handleInvalid)
  
  // VectorIndexer pour identifier catégorielles vs continues
  val vectorIndexer = new VectorIndexer()
                        .setInputCol("featuresVec")
                        .setOutputCol("features")
                        .setMaxCategories(maxCat)
                        .setHandleInvalid(handleInvalid)
  
  val pipeline = new Pipeline()
                    .setStages(Array(stringIndexer, vectorAssembler, vectorIndexer))
  
  return pipeline
}
```

### Avantages de cette approche

1. **Réutilisabilité** : Fonction générique pour différents datasets
2. **Flexibilité** : Gère colonnes textuelles et numériques séparément
3. **Robustesse** : Paramètre handleInvalid pour gérer les erreurs
4. **Standardisation** : Même structure de pipeline pour tous les projets

## 2. Case Class MetaData - Structure des métadonnées

```scala
case class MetaData(name: String, origType: String, colType: String, compRatio: Float, nbDistinctValues: Long)
```

**Champs** :
- **name** : Nom de la colonne
- **origType** : Type Spark original ("StringType", "IntegerType", etc.)
- **colType** : Type simplifié ("textType", "numericType", "otherType")
- **compRatio** : Ratio de complétude (valeurs non-nulles / total)
- **nbDistinctValues** : Nombre de valeurs distinctes

### Exemple d'instance MetaData

```scala
MetaData(
  name = "age",
  origType = "StringType", 
  colType = "textType",
  compRatio = 0.95f,        // 95% des valeurs sont non-nulles
  nbDistinctValues = 3L     // 3 valeurs distinctes: young, middle, senior
)
```

## 3. Fonction whichType - Classification des types

```scala
def whichType(origType: String) = origType match {
  case "StringType" => _text
  case "IntegerType"|"DoubleType" => _numeric  
  case _ => _other
}
```

**Simplification des types Spark** :
- **StringType** → _text ("textType")
- **IntegerType, DoubleType** → _numeric ("numericType")
- **Tout autre type** → _other ("otherType")

**Utilité** : Simplifie la logique de traitement en regroupant les types similaires.

## 4. Fonction MDCompletenessDV - Analyse de qualité

### Signature et objectif

```scala
def MDCompletenessDV(data: DataFrame): DataFrame
```

**Objectif** : Analyser la qualité de chaque colonne d'un DataFrame et retourner les métadonnées.

### Analyse détaillée

#### Étape 1 : Comptage total

```scala
val total_count = data.count()
```

**Fonction** : Nombre total de lignes dans le dataset.

#### Étape 2 : Analyse par colonne

```scala
val res = data.dtypes.map{
  case(colName, colType)=>MetaData(colName, 
                                    colType, 
                                    whichType(colType),
                                    data.filter(col(colName).isNotNull).count.toFloat/total_count,
                                    data.select(colName).distinct().count)
}.toList
```

**Décomposition** :

1. **data.dtypes** : Récupère [(nom_colonne, type_spark)] pour chaque colonne
2. **Pour chaque colonne, calcule** :
   - **colName** : Nom de la colonne
   - **colType** : Type Spark original
   - **whichType(colType)** : Type simplifié
   - **Ratio de complétude** : `filter(isNotNull).count / total_count`
   - **Valeurs distinctes** : `select(colName).distinct().count`

#### Étape 3 : Conversion en DataFrame

```scala
val metadata = res.toDS().toDF()
metadata.persist()  
metadata.count()
return metadata
```

**Optimisations** :
- **persist()** : Met en cache le résultat (sera utilisé plusieurs fois)
- **count()** : Force l'évaluation (lazy evaluation de Spark)

### Exemple de résultat MDCompletenessDV

```scala
+-------------+-----------+-----------+----------+----------------+
|         name|   origType|    colType| compRatio|nbDistinctValues|
+-------------+-----------+-----------+----------+----------------+
|          age| StringType|   textType|       1.0|               3|
|       income| StringType|   textType|       1.0|               3|
|      student| StringType|   textType|       1.0|               2|
|credit_rating| StringType|   textType|       1.0|               2|
| buys_computer| StringType|   textType|       1.0|               2|
|       salary|IntegerType| numericType|      0.85|              50|
+-------------+-----------+-----------+----------+----------------+
```

**Interprétation** :
- **age** : 100% complète, 3 catégories → Bonne pour indexation
- **salary** : 85% complète, 50 valeurs → Variable continue avec valeurs manquantes

## 5. Utilisation pratique des fonctions

### Workflow typique

```scala
// 1. Analyser la qualité des données
val metadata = MDCompletenessDV(rawData)
metadata.orderBy($"compRatio".desc).show()

// 2. Sélectionner les colonnes selon critères de qualité
val textCols = metadata
  .filter($"colType" === "textType" && $"compRatio" > 0.8 && $"nbDistinctValues" < 20)
  .select("name").rdd.map(_.getString(0)).collect()

val numericCols = metadata
  .filter($"colType" === "numericType" && $"compRatio" > 0.7)
  .select("name").rdd.map(_.getString(0)).collect()

// 3. Construire le pipeline automatiquement
val pipeline = AutoPipeline(textCols, numericCols, "target", maxCat=10, "skip")

// 4. Appliquer le preprocessing
val processedData = pipeline.fit(rawData).transform(rawData)
```

### Stratégies de sélection des features

```scala
// Stratégie 1: Colonnes très complètes seulement
val highQualityCols = metadata.filter($"compRatio" > 0.95)

// Stratégie 2: Éviter les colonnes avec trop de catégories
val managibleCategorical = metadata.filter($"colType" === "textType" && $"nbDistinctValues" <= 50)

// Stratégie 3: Colonnes numériques avec peu de valeurs manquantes
val reliableNumeric = metadata.filter($"colType" === "numericType" && $"compRatio" > 0.8)
```

## 6. Extensions et améliorations

### AutoPipeline étendu avec feature selection

```scala
def AutoPipelineWithSelection(data: DataFrame, target: String, 
                             maxCat: Int, handleInvalid: String,
                             minCompleteness: Double = 0.8,
                             maxCategories: Int = 20): Pipeline = {
  
  // Analyse automatique de la qualité
  val metadata = MDCompletenessDV(data)
  
  // Sélection automatique des features
  val textCols = metadata
    .filter($"colType" === "textType" && 
            $"compRatio" >= minCompleteness && 
            $"nbDistinctValues" <= maxCategories &&
            $"name" =!= target)
    .select("name").rdd.map(_.getString(0)).collect()
  
  val numericCols = metadata
    .filter($"colType" === "numericType" && 
            $"compRatio" >= minCompleteness)
    .select("name").rdd.map(_.getString(0)).collect()
  
  // Construction du pipeline
  AutoPipeline(textCols, numericCols, target, maxCat, handleInvalid)
}
```

### Métriques de qualité enrichies

```scala
case class EnrichedMetaData(
  name: String,
  origType: String, 
  colType: String,
  compRatio: Float,
  nbDistinctValues: Long,
  nullCount: Long,
  uniqueRatio: Double,        // Ratio de valeurs uniques
  mostFrequentValue: String,  // Valeur la plus fréquente
  frequency: Long             // Fréquence de la valeur la plus fréquente
)

def enrichedAnalysis(data: DataFrame): DataFrame = {
  val total_count = data.count()
  
  val enrichedRes = data.dtypes.map { case (colName, colType) =>
    val colData = data.select(colName)
    val nullCount = data.filter(col(colName).isNull).count()
    val distinctCount = colData.distinct().count()
    val uniqueRatio = distinctCount.toDouble / total_count
    
    // Valeur la plus fréquente
    val freqAnalysis = colData.groupBy(colName).count()
                              .orderBy(desc("count"))
                              .first()
    
    EnrichedMetaData(
      name = colName,
      origType = colType,
      colType = whichType(colType),
      compRatio = (total_count - nullCount).toFloat / total_count,
      nbDistinctValues = distinctCount,
      nullCount = nullCount,
      uniqueRatio = uniqueRatio,
      mostFrequentValue = freqAnalysis.getString(0),
      frequency = freqAnalysis.getLong(1)
    )
  }.toList
  
  enrichedRes.toDS().toDF()
}
```

## 7. Gestion des cas particuliers

### Gestion des colonnes ID

```scala
def isIdentifierColumn(metadata: MetaData): Boolean = {
  val name = metadata.name.toLowerCase
  val uniqueRatio = metadata.nbDistinctValues.toDouble / metadata.compRatio
  
  // Heuristiques pour détecter les colonnes ID
  name.contains("id") || 
  name.contains("key") || 
  uniqueRatio > 0.95  // Plus de 95% de valeurs uniques
}

// Filtrage automatique des colonnes ID
val nonIdColumns = metadata.filter(!isIdentifierColumn(_))
```

### Gestion des colonnes temporelles

```scala
def whichTypeExtended(origType: String) = origType match {
  case "StringType" => _text
  case "IntegerType"|"DoubleType"|"FloatType"|"LongType" => _numeric
  case "TimestampType"|"DateType" => "temporalType"
  case "BooleanType" => "booleanType"
  case _ => _other
}
```

## 8. Patterns d'utilisation avancés

### Pipeline conditionnel

```scala
def conditionalPipeline(metadata: DataFrame, target: String): Pipeline = {
  val textCols = metadata.filter($"colType" === "textType").select("name").collect().map(_.getString(0))
  val numericCols = metadata.filter($"colType" === "numericType").select("name").collect().map(_.getString(0))
  
  // Adaptation des paramètres selon les données
  val maxCat = if (textCols.length > 10) 50 else 20
  val handleInvalid = if (metadata.filter($"compRatio" < 0.9).count() > 0) "skip" else "error"
  
  AutoPipeline(textCols, numericCols, target, maxCat, handleInvalid)
}
```

### Validation du pipeline

```scala
def validatePipeline(pipeline: Pipeline, data: DataFrame): Boolean = {
  try {
    val model = pipeline.fit(data.limit(100))  // Test sur échantillon
    val transformed = model.transform(data.limit(10))
    
    // Vérifications basiques
    transformed.columns.contains("features") && 
    transformed.columns.contains("label") &&
    transformed.count() > 0
    
  } catch {
    case _: Exception => false
  }
}
```

## Conclusion

Ces fonctions d'automatisation représentent un niveau d'abstraction supérieur dans Spark ML, permettant de :

1. **Standardiser** les pipelines de preprocessing
2. **Automatiser** l'analyse de qualité des données  
3. **Optimiser** la sélection des features
4. **Simplifier** le développement de modèles ML

Elles constituent des briques fondamentales pour construire des systèmes ML robustes et reproductibles à l'échelle industrielle.