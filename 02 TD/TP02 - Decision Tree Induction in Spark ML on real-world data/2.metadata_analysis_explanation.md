# Analyse de Métadonnées avec MDCompletenessDV - Guide Détaillé

## Vue d'ensemble

Ce code implémente un **système d'analyse automatique de la qualité des données** qui examine chaque colonne d'un DataFrame pour extraire des métadonnées essentielles. Il s'agit d'un outil fondamental pour comprendre et préprocesser intelligemment les datasets.

## 1. Case Class MetaData - Structure des métadonnées

### Définition

```scala
case class MetaData(name: String, origType: String, colType: String, compRatio: Float, nbDistinctValues: Long)
```

### Champs détaillés

| Champ | Type | Description | Exemple |
|-------|------|-------------|---------|
| **name** | String | Nom de la colonne | "age" |
| **origType** | String | Type Spark original | "StringType", "IntegerType" |
| **colType** | String | Type simplifié | "textType", "numericType" |
| **compRatio** | Float | Ratio de complétude (0.0 à 1.0) | 0.95 = 95% des valeurs non-nulles |
| **nbDistinctValues** | Long | Nombre de valeurs distinctes | 3 pour {"young", "middle", "senior"} |

### Exemple d'instance

```scala
MetaData(
  name = "age",
  origType = "StringType",
  colType = "textType", 
  compRatio = 1.0f,         // 100% des valeurs sont présentes
  nbDistinctValues = 3L     // 3 catégories distinctes
)
```

### Utilité des métadonnées

- **Sélection de features** : Éliminer les colonnes de mauvaise qualité
- **Configuration de pipeline** : Adapter les paramètres selon les caractéristiques
- **Détection d'anomalies** : Identifier les colonnes problématiques
- **Documentation** : Comprendre la structure du dataset

## 2. Fonction whichType - Classification des types

### Code et logique

```scala
//considers only three types: numeric, textual and other
def whichType(origType: String) = origType match {
  case "StringType" => _text
  case "IntegerType"|"DoubleType" => _numeric
  case _ => _other
}
```

### Mapping détaillé

| Type Spark Original | Type Simplifié | Constante | Utilisation |
|---------------------|----------------|-----------|-------------|
| "StringType" | "textType" | _text | Variables catégorielles à indexer |
| "IntegerType" | "numericType" | _numeric | Variables numériques continues |
| "DoubleType" | "numericType" | _numeric | Variables numériques continues |
| "FloatType" | "otherType" | _other | Types non gérés par défaut |
| "BooleanType" | "otherType" | _other | Types non gérés par défaut |
| "TimestampType" | "otherType" | _other | Types non gérés par défaut |

### Avantages de la simplification

1. **Logique de traitement simplifiée** : 3 catégories au lieu de 10+
2. **Pipeline automatisé** : Décisions basées sur des types génériques
3. **Extensibilité** : Facile d'ajouter de nouveaux types
4. **Robustesse** : Le cas `_` capture tous les types non prévus

### Exemple de classification

```scala
// Dataset avec différents types
val sampleTypes = Array(
  ("name", "StringType"),      // → "textType"
  ("age", "IntegerType"),      // → "numericType"  
  ("salary", "DoubleType"),    // → "numericType"
  ("created_at", "TimestampType"), // → "otherType"
  ("is_active", "BooleanType")     // → "otherType"
)

sampleTypes.foreach { case (name, sparkType) =>
  println(s"$name ($sparkType) → ${whichType(sparkType)}")
}
```

**Sortie** :
```
name (StringType) → textType
age (IntegerType) → numericType
salary (DoubleType) → numericType
created_at (TimestampType) → otherType
is_active (BooleanType) → otherType
```

## 3. Fonction MDCompletenessDV - Analyse principale

### Signature et objectif

```scala
def MDCompletenessDV(data: DataFrame): DataFrame
```

**Objectif** : Analyser la qualité et les caractéristiques de chaque colonne d'un DataFrame et retourner un DataFrame de métadonnées.

### Analyse étape par étape

#### Étape 1 : Comptage total des lignes

```scala
val total_count = data.count()
```

**Fonction** : Obtenir le nombre total de lignes pour calculer les ratios de complétude.

**Exemple** : Si le dataset a 1000 lignes, `total_count = 1000L`

#### Étape 2 : Analyse par colonne

```scala
val res = data.dtypes.map{
  case(colName, colType)=>MetaData(colName, 
                                    colType, 
                                    whichType(colType),
                                    data.filter(col(colName).isNotNull).count.toFloat/total_count,
                                    data.select(colName).distinct().count)
}.toList
```

**Décomposition détaillée** :

##### 2.1 Point de départ : data.dtypes
```scala
data.dtypes
// Retourne : Array[(String, String)] 
// Exemple : Array(("age", "StringType"), ("salary", "IntegerType"))
```

##### 2.2 Transformation map
Pour chaque paire `(colName, colType)`, crée une instance `MetaData` :

**Champ 1 - name** :
```scala
colName  // Nom de la colonne tel quel
```

**Champ 2 - origType** :
```scala
colType  // Type Spark original ("StringType", "IntegerType", etc.)
```

**Champ 3 - colType** :
```scala
whichType(colType)  // Type simplifié via fonction whichType
```

**Champ 4 - compRatio (Ratio de complétude)** :
```scala
data.filter(col(colName).isNotNull).count.toFloat/total_count
```

Décomposition :
- `col(colName)` : Référence à la colonne
- `.isNotNull` : Condition pour valeurs non-nulles  
- `filter(...)` : Filtre les lignes avec valeurs non-nulles
- `.count` : Compte les lignes filtrées
- `.toFloat/total_count` : Calcule le ratio

**Exemple** :
```scala
// Colonne "age" : 950 valeurs non-nulles sur 1000 total
// compRatio = 950.0 / 1000.0 = 0.95 (95%)
```

**Champ 5 - nbDistinctValues (Cardinalité)** :
```scala
data.select(colName).distinct().count
```

Décomposition :
- `select(colName)` : Sélectionne seulement cette colonne
- `.distinct()` : Élimine les doublons
- `.count` : Compte les valeurs uniques

**Exemple** :
```scala
// Colonne "age" avec valeurs : ["young", "middle", "senior", "young", "middle"]
// Distinctes : ["young", "middle", "senior"]  
// nbDistinctValues = 3L
```

#### Étape 3 : Conversion en DataFrame

```scala
val metadata = res.toDS().toDF()
metadata.persist()  
metadata.count()
return metadata
```

**Processus** :
1. **`.toList`** : Convertit le résultat map en List[MetaData]
2. **`.toDS()`** : Crée un Dataset[MetaData] typé
3. **`.toDF()`** : Convertit en DataFrame non-typé
4. **`.persist()`** : Met en cache (sera utilisé plusieurs fois)
5. **`.count()`** : Force l'évaluation immédiate (lazy evaluation)

### Exemple complet d'exécution

#### Dataset d'entrée
```scala
+------+------+-------+-------+-------------+
|   age|income|student| salary|buys_computer|
+------+------+-------+-------+-------------+
| young|  high|     no| 45000 |           no|
|  null|medium|    yes| 35000 |          yes|
|middle|  high|     no|  null |          yes|
|senior|   low|    yes| 25000 |           no|
| young|medium|     no| 40000 |          yes|
+------+------+-------+-------+-------------+
```

#### Résultat MDCompletenessDV
```scala
+-------------+-----------+-----------+----------+----------------+
|         name|   origType|    colType| compRatio|nbDistinctValues|
+-------------+-----------+-----------+----------+----------------+
|          age| StringType|   textType|       0.8|               3|
|       income| StringType|   textType|       1.0|               3|  
|      student| StringType|   textType|       1.0|               2|
|       salary|IntegerType| numericType|       0.8|               4|
| buys_computer| StringType|   textType|       1.0|               2|
+-------------+-----------+-----------+----------+----------------+
```

#### Calculs détaillés

**Colonne "age"** :
- Valeurs : ["young", null, "middle", "senior", "young"]
- Non-nulles : 4 sur 5 → `compRatio = 4/5 = 0.8`
- Distinctes : ["young", "middle", "senior"] → `nbDistinctValues = 3`

**Colonne "salary"** :
- Valeurs : [45000, 35000, null, 25000, 40000]  
- Non-nulles : 4 sur 5 → `compRatio = 4/5 = 0.8`
- Distinctes : [45000, 35000, 25000, 40000] → `nbDistinctValues = 4`

## 4. Utilisation pratique

### Analyse de qualité basique

```scala
// Analyser un dataset
val rawData = spark.read.format("csv")
  .option("header", "true")
  .option("inferSchema", "true")
  .load("path/to/data.csv")

val metadata = MDCompletenessDV(rawData)

// Afficher les métadonnées triées par complétude
metadata.orderBy($"compRatio".desc).show()
```

### Sélection intelligente de features

```scala
// Features de haute qualité seulement
val highQualityFeatures = metadata
  .filter($"compRatio" >= 0.9)  // 90%+ de complétude
  .filter($"colType" === "textType" || $"colType" === "numericType")
  .select("name").rdd.map(_.getString(0)).collect()

println(s"Features sélectionnées : ${highQualityFeatures.mkString(", ")}")
```

### Détection de colonnes problématiques

```scala
// Colonnes avec beaucoup de valeurs manquantes
val incompleteColumns = metadata
  .filter($"compRatio" < 0.5)
  .select("name", "compRatio")

println("Colonnes problématiques :")
incompleteColumns.show()

// Colonnes avec trop de catégories (possibles ID)
val suspiciousColumns = metadata
  .filter($"colType" === "textType" && $"nbDistinctValues" > 100)
  .select("name", "nbDistinctValues")

println("Colonnes suspectes (possibles identifiants) :")
suspiciousColumns.show()
```

### Configuration automatique de pipeline

```scala
def smartPipelineConfig(metadata: DataFrame): (Array[String], Array[String], Int) = {
  // Colonnes textuelles de bonne qualité avec cardinalité raisonnable
  val textCols = metadata
    .filter($"colType" === "textType" && 
            $"compRatio" > 0.8 && 
            $"nbDistinctValues" <= 20)
    .select("name").rdd.map(_.getString(0)).collect()
  
  // Colonnes numériques de bonne qualité  
  val numericCols = metadata
    .filter($"colType" === "numericType" && $"compRatio" > 0.7)
    .select("name").rdd.map(_.getString(0)).collect()
  
  // MaxCategories basé sur la distribution des cardinalités
  val avgCardinality = metadata
    .filter($"colType" === "textType")
    .agg(avg($"nbDistinctValues")).collect()(0).getDouble(0)
  val maxCat = math.max(10, avgCardinality.toInt * 2)
  
  (textCols, numericCols, maxCat)
}

// Usage
val (textCols, numericCols, maxCat) = smartPipelineConfig(metadata)
val pipeline = AutoPipeline(textCols, numericCols, "target", maxCat, "skip")
```

## 5. Interprétation des métriques

### CompRatio (Ratio de complétude)

| Valeur | Interprétation | Action recommandée |
|--------|----------------|--------------------|
| 1.0 | Parfait - Aucune valeur manquante | ✅ Utiliser tel quel |
| 0.9 - 0.99 | Très bon - Peu de valeurs manquantes | ✅ Utiliser avec précaution |
| 0.7 - 0.89 | Moyen - Valeurs manquantes notables | ⚠️ Imputation ou exclusion |
| 0.5 - 0.69 | Problématique - Beaucoup de manques | ❌ Probablement à exclure |
| < 0.5 | Critique - Majorité de valeurs manquantes | ❌ À exclure définitivement |

### NbDist