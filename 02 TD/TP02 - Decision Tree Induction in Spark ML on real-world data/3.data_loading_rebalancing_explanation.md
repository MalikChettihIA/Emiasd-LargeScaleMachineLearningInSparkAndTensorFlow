# Chargement de Données CSV et Rééquilibrage de Classes - Guide Détaillé

## Vue d'ensemble

Ce code illustre une séquence typique de préparation de données pour un projet de machine learning : **chargement d'un fichier CSV**, **analyse de la distribution des classes**, et **rééquilibrage artificiel** pour corriger un déséquilibre de classes.

## 1. Chargement des données CSV

### Code analysé

```scala
val raw_data = spark.read.format("csv")
            .option("header", "true")
            .option("inferSchema", "true")
            .load(dbfsDir+"/loan.csv")
            .persist()
raw_data.count()
```

### Décomposition étape par étape

#### Étape 1 : Configuration du lecteur CSV

```scala
spark.read.format("csv")
```
- **spark.read** : Point d'entrée pour lire des données dans Spark
- **format("csv")** : Spécifie le format de fichier (CSV)

#### Étape 2 : Configuration des options

```scala
.option("header", "true")
.option("inferSchema", "true")
```

**Options détaillées** :

| Option | Valeur | Description | Impact |
|--------|--------|-------------|--------|
| **header** | "true" | Première ligne contient les noms de colonnes | Colonnes nommées automatiquement |
| **inferSchema** | "true" | Détection automatique des types de données | String/Integer/Double assignés intelligemment |

**Sans ces options** :
```scala
// Sans header="true" : colonnes nommées _c0, _c1, _c2...
// Sans inferSchema="true" : toutes les colonnes en StringType
```

**Avec ces options** :
```scala
// header="true" : colonnes nommées age, income, loan_amount...  
// inferSchema="true" : age:IntegerType, income:DoubleType, name:StringType...
```

#### Étape 3 : Chargement du fichier

```scala
.load(dbfsDir+"/loan.csv")
```
- **dbfsDir** : Variable contenant le chemin du répertoire (ex: "/FileStore/tables/")
- **Chemin complet** : "/FileStore/tables/loan.csv"
- **load()** : Exécute la lecture et retourne un DataFrame

#### Étape 4 : Optimisation avec cache

```scala
.persist()
```
**Fonction** : Met le DataFrame en cache mémoire pour réutilisations multiples.

**Avantages** :
- **Performance** : Évite de relire le fichier à chaque opération
- **Efficacité** : Particulièrement utile si le DataFrame sera utilisé plusieurs fois
- **Réactivité** : Accès instantané aux données pour l'exploration

#### Étape 5 : Force l'évaluation

```scala
raw_data.count()
```
**Rôle** : Force l'évaluation immédiate du DataFrame (Spark utilise la lazy evaluation).

**Pourquoi nécessaire** :
- Spark ne charge pas réellement les données tant qu'une action n'est pas demandée
- `count()` est une action qui déclenche la lecture effective du fichier
- Garantit que `.persist()` est effectif avant les opérations suivantes

### Résultat attendu

```scala
// Exemple de sortie
raw_data.count()
// res0: Long = 15000   (15000 lignes chargées)

raw_data.printSchema()
// root
//  |-- CustomerID: integer (nullable = true)
//  |-- Age: integer (nullable = true)
//  |-- Income: double (nullable = true)
//  |-- LoanAmount: double (nullable = true)
//  |-- Approved: string (nullable = true)

raw_data.show(5)
// +----------+---+--------+----------+--------+
// |CustomerID|Age|  Income|LoanAmount|Approved|
// +----------+---+--------+----------+--------+
// |         1| 35|45000.0 |   12000.0|     yes|
// |         2| 28|38000.0 |   18000.0|      no|
// |         3| 42|67000.0 |   25000.0|     yes|
// +----------+---+--------+----------+--------+
```

## 2. Définition de la variable cible

### Code analysé

```scala
val target = "Approved"
```

**Fonction** : Définit la colonne qui servira de variable cible (label) pour le modèle de classification.

**Avantages de la centralisation** :
- **Maintenance** : Un seul endroit à modifier si le nom change
- **Cohérence** : Même nom utilisé partout dans le code
- **Documentation** : Rend explicite quelle est la variable à prédire

## 3. Analyse de la distribution initiale

### Code analysé

```scala
raw_data.groupBy(target).count().show()
```

### Décomposition

**Opérations** :
1. **groupBy(target)** : Groupe les lignes par valeur de la colonne "Approved"
2. **count()** : Compte le nombre de lignes dans chaque groupe
3. **show()** : Affiche le résultat sous forme de tableau

### Exemple de résultat

```scala
+--------+-----+
|Approved|count|
+--------+-----+
|     yes| 2100|  // 14% des cas (minoritaire)
|      no|12900|  // 86% des cas (majoritaire)
+--------+-----+
```

### Interprétation

**Problème identifié** : **Déséquilibre de classes sévère**
- **Classe majoritaire** : "no" (86%)
- **Classe minoritaire** : "yes" (14%)
- **Ratio** : 6:1 (déséquilibré)

**Impact sur le ML** :
- **Biais de prédiction** : Modèle prédit massivement la classe majoritaire
- **Accuracy trompeur** : 86% d'accuracy en prédisant toujours "no"
- **Rappel faible** : Classe minoritaire mal détectée

## 4. Rééquilibrage artificiel des classes

### Code analysé

```scala
import org.apache.spark.sql.functions.{rand, when, col}
val data = raw_data.withColumn("t", when(rand>0.5,0).otherwise(1)).drop(target).withColumnRenamed("t",target).drop("t")
data.groupBy(target).count().show()
```

### Décomposition détaillée

#### Import des fonctions nécessaires

```scala
import org.apache.spark.sql.functions.{rand, when, col}
```

**Fonctions importées** :
- **rand** : Génère des nombres aléatoires entre 0 et 1
- **when** : Structure conditionnelle (équivalent à if-then-else)
- **col** : Référence à une colonne du DataFrame

#### Étape 1 : Création d'une colonne temporaire aléatoire

```scala
.withColumn("t", when(rand>0.5,0).otherwise(1))
```

**Logique** :
- **rand** : Nombre aléatoire entre 0.0 et 1.0
- **rand>0.5** : Condition vraie ~50% du temps
- **when(condition, 0)** : Si condition vraie → valeur 0
- **otherwise(1)** : Sinon → valeur 1

**Résultat** : Nouvelle colonne "t" avec des 0 et 1 distribués aléatoirement 50/50.

**Exemple** :
```scala
// Avant
+----------+---+--------+--------+
|CustomerID|Age|  Income|Approved|
+----------+---+--------+--------+
|         1| 35|45000.0 |     yes|
|         2| 28|38000.0 |      no|

// Après withColumn("t", ...)
+----------+---+--------+--------+---+
|CustomerID|Age|  Income|Approved|  t|
+----------+---+--------+--------+---+
|         1| 35|45000.0 |     yes|  0|  // rand généré : 0.3 < 0.5 → 0
|         2| 28|38000.0 |      no|  1|  // rand généré : 0.7 > 0.5 → 1
```

#### Étape 2 : Suppression de l'ancienne colonne cible

```scala
.drop(target)
```
**Fonction** : Supprime la colonne "Approved" orig